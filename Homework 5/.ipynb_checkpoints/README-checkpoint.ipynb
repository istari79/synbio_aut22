{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "medium-teens",
   "metadata": {},
   "source": [
    "# HW5: Due on Fri 12/14 at the end of class.\n",
    "\n",
    "## Setup \n",
    "\n",
    "We will be using Python 3 and Juptyer Notebook (or Jupyter Lab).  The Python packages we will be using are: \n",
    "+ pandas\n",
    "+ numpy\n",
    "+ matplotlib\n",
    "+ seaborn \n",
    "+ scikit-learn\n",
    "+ keras (with tensorflow 2.4) (only needed for Problem 3)\n",
    "\n",
    "Please run *check_pacakges.ipynb* to make sure everything installed correctly. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this homework, you will be examining 5' UTR sequence features, predicting 5' UTR mean ribosomal load (MRL) using linear regression, and engineering 5' UTR sequences using your regression model and the pre-trained deep learning neural network Optimus 5-Prime. \n",
    "\n",
    "_Optional reading:_\n",
    "\n",
    "+ [General 5' UTR review](https://www.hindawi.com/journals/ijg/2012/475731/)\n",
    "\n",
    "+ [Linear Regression](https://machinelearningmastery.com/linear-regression-for-machine-learning/) (we will be training ordinary least squares multiple linear regression models)\n",
    "\n",
    "+ [Optimus 5-Prime](https://www.nature.com/articles/s41587-019-0164-5) Optimus 5-Prime is a convolutional neural network model for predicting MRL for a 5' UTR sequence trained on library of 280000 random 5'UTR sequences. \n",
    "\n",
    "\n",
    "## 1. Examining upstream 5'UTR Elements and MRL\n",
    "\n",
    "For this question, you will be looking at the affect of upstream starts, stops, and open reading frames on MRL.  The dataset you will be using is included on canvas, *mrl_dataset.csv*.  This is part of one of the Optimus 5-Prime datasets of random 50 nucleotide 5' UTR sequences and their ribosomal loading measurements, which you will be analyzing.\n",
    "\n",
    "+ *1.1: Graphing the effects of upstream starts (5 points)* Complete the code in the provided Jupyter notebook *1_examine_5_utr.ipynb* to create a graph showing average MRL based on start codon position reading frame in the UTR sequence. Generate these graphs for start codons ATG, CTG, and GTG.  Summarize the effects of in frame vs. out of frame starts, does this match what you would expect based on your knowledge of 5' UTRs?\n",
    "\n",
    "\n",
    "+ *1.2: Graphing the effects of upstream stops (5 points)* Adapt your code from above to graph average MRL for stop codons TAA, TAG, TGA.  What effects on MRL do pstream stop codons appear to have (if any). Does this match your expectations?\n",
    "\n",
    "## 2. Linear Regression MRL Model\n",
    "\n",
    "Now you will be training a simple k-mer linear regression model to predict MRL based on k-mer counts in the 50 nt sequence.  \n",
    "\n",
    "+ *2.1: Linear regression without position information (5 points)*  First, you will train a linear regression model for k-mers without position. Complete the functions in *2_linear_regression.ipynb*.  You will be training a [scikit-learn linear regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) (though if you want to, feel free to explore other models from scikit or implement your own).  Try training models for $k= 1, 2,...5$.  How does MSE and $r^2$ change as $k$ increases?  Naively, one might assume that increasing $k$ will allow your model to capture more complex features of the data and in turn train a better model. Do you see this trend in your models? Write your answer in the markup cell below.\n",
    "\n",
    "\n",
    "+ *2.2: Linear regression with position information (5 points)* Now, train a linear regression model for k-mers with reading frame information using $k=1,2,...5$.  Previously the features used were the count of each k-mer in the sequence.  Now, you will count each k-mer occurring in each possible of the three reading frames (so, there will be three times the features as before). An example of the difference between k-mer and k-mer with position features for $k=1$ is shown below. Previously, there were four features with $k=1$, but with reading frame position information there are twelve features.\n",
    "\n",
    "![](images/demo_kmer.png \"Example for $k=1$\")\n",
    "\n",
    "Does including position information improve prediction performance? Additionally, note that training and test performances deviate in both the MSE and $r^2$ metrics at the higher values of $k$. This deviation is more strongly apparent in the position model vs. the positionless model. What is the name of this pathology (i.e. systematic error in what the model has learned); why does it make sense to see this occur only in your higher-$k$ models; and why is this behavior more strongly evident in the position model vs. the positionless model? Write your answer in the markup cell in the corresponding notebook.\n",
    "\n",
    "## 3.  Random mutagenesis for sequence design. Grad students only!\n",
    "\n",
    "Finally, you will be using your best performing linear regression model and the pre-trained Optimus 5-Prime model to engineer high MRL 5' UTR sequences.  \n",
    "\n",
    "+ *3.1: Engineering sequences using linear regression model (5 points)*  Using your best performing linear regression model, use random mutagenesis staring with a random 50 nt sequence to make high MRL sequences. Skeleton code is provided in *3_sequence_design.ipynb*.  Try engineer 3 sequences with high MRL values. \n",
    "\n",
    "\n",
    "+ *3.2: Enginering sequences using Optimus 5-Prime (5 points)*  Complete the rest of the skeleton code in *3_sequence_design.ipynb*.  Similar to *3.1* try engineering 3 sequences with high MRL values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3834cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
