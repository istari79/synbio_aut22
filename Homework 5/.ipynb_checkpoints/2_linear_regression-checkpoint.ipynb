{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1K-BHqEuzchk"
   },
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "import numpy\n",
    "import pandas\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOjouB6qzchn"
   },
   "source": [
    "## 2. Linear Regression MRL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47OHBEJ9zcho"
   },
   "source": [
    "### 2.1 Linear Regression without position information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVDMCDekzchp"
   },
   "outputs": [],
   "source": [
    "################### Provided functions, you will not need to edit these ##################################3\n",
    "\n",
    "def load_data():\n",
    "    data = pandas.read_csv('mrl_dataset.csv', index_col=0)\n",
    "    # Retain only 280k with the highest number of reads\n",
    "    data.sort_values('total_reads', inplace=True, ascending=False)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    # Random shuffle\n",
    "    data = data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "    # Split into training and test datasets\n",
    "    data_train = data.iloc[2000:10000]\n",
    "    data_test = data.iloc[:2000]\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "#load train and test datasets \n",
    "data_train, data_test = load_data()\n",
    "print(f'Number of sequences in training set: {data_train.shape[0]}')\n",
    "print(f'Number of sequences in test set:     {data_test.shape[0]}')\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UtvK4k19zchp"
   },
   "outputs": [],
   "source": [
    "################ You will need to edit these functions for 2.1 ############################\n",
    "\n",
    "# Note: if confused about intended usage of these functions, see cell below this one where they are implemented.\n",
    "\n",
    "###############################\n",
    "def create_kmer_dict(k):\n",
    "    # Create dictionary where the keys are kmer strings, and the value for each key is an integer index,\n",
    "    #  i.e. kmer_dict.values should return an np.array equal to range(4**k) This function will be used to\n",
    "    #  create consistent indexing for kmers across the other functions in this cell & notebook.\n",
    "    bases = ['A', 'C', 'G', 'T']\n",
    "\n",
    "    # TODO: your code goes here\n",
    "\n",
    "    return kmer_dict\n",
    "\n",
    "def count_kmers(s, kmer_dict,k):\n",
    "    #creates feature matrix for a list of sequences \n",
    "    #input: list of utr sequences s \n",
    "    #output: the filled feature matrix counts \n",
    "    bases = ['A', 'C', 'G', 'T']\n",
    "    counts = numpy.zeros((len(s), len(bases)**k)) \n",
    "\n",
    "    #TODO: fill in counts matrix for the list of sequences s (see example image on Canvas)\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def prepare_data_for_linear_regression(data_train, data_test, kmer_dict, k):\n",
    "    # Preprocess training and test data\n",
    "    #take in data_train and data_test pandas dataframes, k of the current model \n",
    "    training_sequences = data_train['utr'].values\n",
    "    test_sequences = data_test['utr'].values\n",
    "    train_y = data_train['rl'].values\n",
    "    test_y = data_test['rl'].values\n",
    "    \n",
    "    #TODO: use count_kmers to make numpy matrices for the training and test sequences. Should have\n",
    "    #  dims (data_train.shape[0] x len(kmer_dict)), (data_test.shape[0] x len(kmer_dict))\n",
    "    \n",
    "    #TODO: Add a bias term to the train and test matrices (add a column of all 1's to each X matrix)\n",
    "    # for example, use numpy.hstack. HINT: Use print statements to confirm you have the right dimensions!\n",
    "    \n",
    "    return train_X_mat, train_y, test_X_mat, test_y\n",
    "\n",
    "def solve_linear_regression_sklearn(train_X_mat, train_y):\n",
    "    #return the trained sckit learn linear regression model \n",
    "\n",
    "    model = LinearRegression()\n",
    "    # TODO: Solve equation system using sci-kit learn LinearRegression model. This should be a one-liner!\n",
    "    #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\n",
    "    return model\n",
    "\n",
    "def calculate_mse(predicted, true):\n",
    "    #TODO return the mean squared error (MSE) for the predicted and true MRL values \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czeQf-uUzchq"
   },
   "outputs": [],
   "source": [
    "#2.1\n",
    "################### Don't need to edit these, just run this cell ####################################\n",
    "# Run k-mer linear regressions for k=1-6 with positions, plotting the results & calculating MSE and r\n",
    "ks = []\n",
    "train_mse = []\n",
    "train_r = []\n",
    "test_mse = []\n",
    "test_r = []\n",
    "\n",
    "for k in range(1, 6): \n",
    "    #for each k, make a linear model without position, get the mse and r^2\n",
    "    print (\"K = \", k)\n",
    "    #1) Create the k-mer dictionary \n",
    "    kmer_dict = create_kmer_dict(k)\n",
    "    print (len(kmer_dict))\n",
    "    #2) Count k-mers in the sequences in the train and test sets, represent them as matrices to solve the linear equation  \n",
    "    train_X_mat, train_y, test_X_mat, test_y = prepare_data_for_linear_regression(data_train, data_test, kmer_dict, k)\n",
    "    #3) Do the linear regression, get the weights \n",
    "    model = solve_linear_regression_sklearn(train_X_mat, train_y)\n",
    "    #4) Make linear regression predictions for train and test set\n",
    "    train_pred = model.predict(train_X_mat)\n",
    "    test_pred = model.predict(test_X_mat)\n",
    "    #6) Graph predictions, calculate MSE and r of the train and test sets\n",
    "    plt.scatter(train_y, train_pred, s=5, alpha=0.1)\n",
    "    plt.xlim(0, 10)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.xlabel('Observed MRL')\n",
    "    plt.ylabel('Predicted MRL')\n",
    "    plt.title(\"Training Predictions k = \" + str(k))\n",
    "    plt.show()\n",
    "    print (\"Train r^2: \", model.score(train_X_mat,  train_y))\n",
    "    print (\"Train MSE: \", calculate_mse(train_pred, train_y))\n",
    "    plt.scatter(test_y, test_pred, s=5, alpha=0.1)\n",
    "    plt.xlim(0, 10)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.xlabel('Observed MRL')\n",
    "    plt.ylabel('Predicted MRL')\n",
    "    plt.title(\"Test Predictions k = \" + str(k))\n",
    "    plt.show()\n",
    "    print (\"Test r^2: \", model.score(test_X_mat,  test_y))\n",
    "    print (\"Test MSE: \", calculate_mse(test_pred, test_y))\n",
    "    #stats for overall graphs \n",
    "    ks.append(k)\n",
    "    train_mse.append(calculate_mse(train_pred, train_y))\n",
    "    train_r.append(model.score(train_X_mat,  train_y))\n",
    "    test_mse.append(model.score(test_X_mat,  test_y))\n",
    "    test_r.append(r2(test_pred, test_y ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpMDCgBizchs"
   },
   "outputs": [],
   "source": [
    "################### Don't need to edit these, just run this cell ####################################\n",
    "\n",
    "#2.1 \n",
    "#### plotting overall mse and r performacnce based on k\n",
    "plt.plot(ks, train_mse, label = 'Train MSE')\n",
    "plt.plot(ks, test_mse, label = \"Test MSE\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"k-mer Linear Regression without position MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#plotting overall r performacnce based on k\n",
    "plt.plot(ks, train_r, label = 'Train r^2')\n",
    "plt.plot(ks, test_r, label = \"Test r^2\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"r^2\")\n",
    "plt.title(\"k-mer Linear Regression without position $r^2$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4M5MEQYXzchs"
   },
   "source": [
    "How does MSE and $r^2$ change as $k$ increases? Naively, one might assume that increasing $k$ will allow your model to capture more complex features of the data and in turn train a better model. Do you see this trend in your models? Write your answer in the markup cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toisZx2vzcht"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOf5YBqxzchu"
   },
   "source": [
    "### 2.2 Linear Regression with position information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "725h4F-9zchu"
   },
   "outputs": [],
   "source": [
    "################ You will need to edit these functions for 2.2 ############################\n",
    "\n",
    "def count_kmers_with_positions(s, kmer_dict, k):\n",
    "    #for every sequence in list s, count the numbe of kmers with position information (see example in instructions)\n",
    "    stride = 3\n",
    "    counts = numpy.zeros((len(s), 4**k * stride))\n",
    "    #TODO: fill in counts matrix\n",
    "    return counts\n",
    "\n",
    "def prepare_data_for_linear_regression_with_positions(data_train, data_test, kmer_dict, k):\n",
    "    # Preprocess training and test data\n",
    "    #take in data_train and data_test pandas dataframes, k of the current model \n",
    "    training_sequences = data_train['utr'].values\n",
    "    test_sequences = data_test['utr'].values\n",
    "    train_y = data_train['rl'].values\n",
    "    test_y = data_test['rl'].values\n",
    "    \n",
    "    #TODO: use count_kmers_with_positions to process the training and test sets \n",
    "    \n",
    "    return train_X_mat, train_y, test_X_mat, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eQ6NJ4vzchv"
   },
   "outputs": [],
   "source": [
    "#2.2\n",
    "\n",
    "################### Don't need to edit these, just run this cell ####################################\n",
    "\n",
    "# Run k-mer linear regressions for k=1-6 with positions, plotting the results & calculating MSE and r\n",
    "ks = []\n",
    "train_mse = []\n",
    "train_r = []\n",
    "test_mse = []\n",
    "test_r = []\n",
    "\n",
    "for k in range(1, 6): \n",
    "    #for each k, make a linear model without position, get the mse and r^2\n",
    "    print (\"K = \", k)\n",
    "    #1) Create the k-mer dictionary \n",
    "    kmer_dict = create_k_mer_dict(k)\n",
    "    #2) Count k-mers in the sequences in the train and test sets, represent them as matrices to solve the linear equation  \n",
    "    train_X_mat, train_y, test_X_mat, test_y = prepare_data_for_linear_regression_with_positions(data_train, data_test, kmer_dict, k)\n",
    "    #3) Do the linear regression, get the weights \n",
    "    model = solve_linear_regression_sklearn(train_X_mat, train_y)\n",
    "    #4) Make linear regression predictions for train and test set\n",
    "    train_pred = model.predict(train_X_mat)\n",
    "    test_pred = model.predict(test_X_mat)\n",
    "    #6) Graph predictions, calculate MSE and r of the train and test sets\n",
    "    plt.scatter(train_y, train_pred, s=5, alpha=0.1)\n",
    "    plt.xlim(0, 10)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.xlabel('Observed MRL')\n",
    "    plt.ylabel('Predicted MRL')\n",
    "    plt.title(\"Training Predictions k = \" + str(k))\n",
    "    plt.show()\n",
    "    print (\"Train r^2: \", model.score(train_X_mat,  train_y))\n",
    "    print (\"Train MSE: \", calculate_mse(train_pred, train_y))\n",
    "    plt.scatter(test_y, test_pred, s=5, alpha=0.1)\n",
    "    plt.xlim(0, 10)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.xlabel('Observed MRL')\n",
    "    plt.ylabel('Predicted MRL')\n",
    "    plt.title(\"Test Predictions k = \" + str(k))\n",
    "    plt.show()\n",
    "    print (\"Test r^2: \", model.score(test_X_mat,  test_y))\n",
    "    print (\"Test MSE: \", calculate_mse(test_pred, test_y))\n",
    "    #stats for overall graphs \n",
    "    ks.append(k)\n",
    "    train_mse.append(calculate_mse(train_pred, train_y))\n",
    "    train_r.append(r2(train_pred, train_y ))\n",
    "    test_mse.append(calculate_mse(test_pred, test_y))\n",
    "    test_r.append(r2(test_pred, test_y ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QG3D0Mdzchv"
   },
   "outputs": [],
   "source": [
    "################### Don't need to edit these, just run this cell ####################################\n",
    "\n",
    "#2.2 \n",
    "#### plotting overall mse and r performacnce based on k\n",
    "\n",
    "plt.plot(ks, train_mse, label = 'Train MSE')\n",
    "plt.plot(ks, test_mse, label = \"Test MSE\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"k-mer Linear Regression with positions MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#plotting overall r performacnce based on k\n",
    "plt.plot(ks, train_r, label = 'Train MSE')\n",
    "plt.plot(ks, test_r, label = \"Test MSE\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"r^2\")\n",
    "plt.title(\"k-mer Linear Regression with positions r^2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPiNo2ZVzchw"
   },
   "source": [
    "Does including position information improve prediction performance? Additionally, note that training and test performances deviate in both the MSE and $r^2$ metrics at the higher values of $k$. This deviation is more strongly apparent in the position model vs. the positionless model. What is the name of this pathology (i.e. systematic error in what the model has learned); why does it make sense to see this occur only in your higher-$k$ models; and why is this behavior more strongly evident in the position model vs. the positionless model? Write your answer in the markup cell below."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f43fd126eed755861b659d0a5657750b9949fa700013cfbc2049902c847cb29b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
